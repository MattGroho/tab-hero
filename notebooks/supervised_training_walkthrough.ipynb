{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0000001",
   "metadata": {},
   "source": [
    "# Tab Hero - Supervised Training Walkthrough\n",
    "\n",
    "Quick notebook showing how the model is trained end-to-end: tokenization, dataset, model, one training step, and reading back a checkpoint.\n",
    "\n",
    "The model is an encoder-decoder transformer. The encoder processes a mel spectrogram; the decoder generates a sequence of note tokens autoregressively. Each note is four tokens: time delta, fret combination, modifier flags (HOPO/TAP/star power), duration.\n",
    "\n",
    "Run from the `notebooks/` directory. Assumes `tab_hero` is installed (`pip install -e .`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.join(os.path.dirname(os.getcwd()), \"src\"))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0000003",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Each note becomes four consecutive tokens, `[TIME_DELTA, FRET, MODIFIER, DURATION]`, so the full sequence looks like `[BOS, T, F, M, D, T, F, M, D, ..., EOS]`. The tokenizer handles quantization (10 ms bins for time, 50 ms for duration) and the fret bitmask encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0000005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.join(os.path.dirname(os.getcwd()), \"src\"))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"torch {torch.__version__}, device={DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0000005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tab_hero.dataio.tokenizer import ChartTokenizer\n",
    "\n",
    "tok = ChartTokenizer()\n",
    "print(f\"vocab size: {tok.vocab_size}, tokens per note: {tok.tokens_per_note}\")\n",
    "\n",
    "# encode a single note and decode it back\n",
    "t = tok.encode_time_delta(350.0)\n",
    "f = tok.encode_frets([2, 4])\n",
    "m = tok.encode_modifiers(is_hopo=True, is_tap=False, is_star_power=False)\n",
    "d = tok.encode_duration(200.0)\n",
    "print(f\"quad: {t} {f} {m} {d}  →  '{tok.id_to_token[t]} {tok.id_to_token[f]} {tok.id_to_token[m]} {tok.id_to_token[d]}'\")\n",
    "print(f\"decoded: {tok.decode_time_delta(t)} ms, frets={tok.decode_frets(f)}, hopo={tok.decode_modifiers(m)[0]}, dur={tok.decode_duration(d)} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0000006",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Songs are preprocessed into `.tab` files (`scripts/preprocess.py`) which bundle the mel spectrogram and token sequence together. At training time we use `ChunkedTabDataset` because full songs are too long to process in one shot - they get sliced into overlapping windows. SpecAugment is applied on-the-fly when `training=True`.\n",
    "\n",
    "If you don't have preprocessed data yet:\n",
    "```bash\n",
    "python scripts/preprocess.py --input_dir data/sample --output_dir data/sample/processed\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0000007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "from tab_hero.dataio.chunked_dataset import ChunkedTabDataset, chunked_collate_fn\n",
    "\n",
    "DATA_DIR = \"../data/sample/processed\"\n",
    "\n",
    "if not list(Path(DATA_DIR).glob(\"*.tab\")):\n",
    "    print(\"no .tab files found — run preprocess.py first\")\n",
    "else:\n",
    "    dataset = ChunkedTabDataset(\n",
    "        data_dir=DATA_DIR,\n",
    "        split=None,\n",
    "        max_mel_frames=4096,\n",
    "        max_token_length=2048,\n",
    "        chunk_overlap_frames=256,\n",
    "        training=True,\n",
    "    )\n",
    "    loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=chunked_collate_fn)\n",
    "    batch = next(iter(loader))\n",
    "\n",
    "    print(f\"{len(dataset)} chunks\")\n",
    "    print(f\"audio : {batch['audio_embeddings'].shape}  (batch, frames, n_mels)\")\n",
    "    print(f\"tokens: {batch['note_tokens'].shape}  (batch, seq_len)\")\n",
    "    print(f\"difficulty: {batch['difficulty_id']}, instrument: {batch['instrument_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0000009",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "The encoder is a small Conv1D stack that downsamples the mel frames 4x before passing them to the decoder via cross-attention. The decoder is a standard causal transformer with RoPE positional encoding and instrument/difficulty conditioning embeddings.\n",
    "\n",
    "The config below is smaller than the largest model variant, but runs fine on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0000010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tab_hero.model.chart_transformer import ChartTransformer\n",
    "\n",
    "model = ChartTransformer(\n",
    "    vocab_size=tok.vocab_size,\n",
    "    audio_input_dim=128,\n",
    "    encoder_dim=256,\n",
    "    decoder_dim=256,\n",
    "    n_decoder_layers=4,\n",
    "    n_heads=8,\n",
    "    ffn_dim=1024,\n",
    "    max_seq_len=2048,\n",
    "    dropout=0.1,\n",
    "    audio_downsample=4,\n",
    "    use_flash=False,\n",
    "    use_rope=True,\n",
    "    gradient_checkpointing=False,\n",
    ").to(DEVICE)\n",
    "\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total:,} parameters ({model.get_num_params(non_embedding=True):,} non-embedding)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0000011",
   "metadata": {},
   "source": [
    "## Forward pass\n",
    "\n",
    "The model uses teacher forcing during training - the decoder gets ground-truth tokens as input (shifted right) rather than its own predictions. At random init, loss should be around `log(vocab_size) ≈ 6.6`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0000012",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio  = torch.randn(2, 512, 128, device=DEVICE)\n",
    "tokens = torch.randint(3, tok.vocab_size, (2, 64), device=DEVICE)\n",
    "tokens[:, 0] = tok.bos_token_id\n",
    "tokens[:, -1] = tok.eos_token_id\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(audio, tokens,\n",
    "                difficulty_id=torch.tensor([3, 1], device=DEVICE),\n",
    "                instrument_id=torch.tensor([0, 1], device=DEVICE))\n",
    "\n",
    "print(f\"logits: {out['logits'].shape}\")\n",
    "print(f\"loss:   {out['loss'].item():.4f}  (expected ~{np.log(tok.vocab_size):.2f} at random init)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0000014",
   "metadata": {},
   "source": [
    "## One training step\n",
    "\n",
    "The Trainer (in `tab_hero/training/trainer.py`) wraps all of this, but it's useful to see what one step actually looks like. Default optimizer is AdamW with β₂=0.95 (standard for transformers), cosine LR schedule with linear warmup, BF16 mixed precision, and gradient clipping at 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0000015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01, betas=(0.9, 0.95))\n",
    "\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "out = model(audio, tokens)\n",
    "out[\"loss\"].backward()\n",
    "grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "optimizer.step()\n",
    "\n",
    "print(f\"loss: {out['loss'].item():.4f}, grad norm: {grad_norm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0000016",
   "metadata": {},
   "source": [
    "## Running actual training\n",
    "\n",
    "```bash\n",
    "python scripts/train.py\n",
    "\n",
    "# smoke-test on 100 samples\n",
    "python scripts/train.py data.max_samples=100 training.max_epochs=5\n",
    "\n",
    "# resume\n",
    "python scripts/train.py training.resume_checkpoint=last_model.pt\n",
    "```\n",
    "\n",
    "Progress is written to `training_progress.log`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0000017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "LOG_PATH = Path(\"../training_progress.log\")\n",
    "\n",
    "if not LOG_PATH.exists():\n",
    "    print(\"no log yet — run training first\")\n",
    "else:\n",
    "    steps, losses = [], []\n",
    "    for line in LOG_PATH.read_text().splitlines():\n",
    "        m = re.search(r\"Step (\\d+): loss=([\\d.]+)\", line)\n",
    "        if m:\n",
    "            steps.append(int(m.group(1)))\n",
    "            losses.append(float(m.group(2)))\n",
    "\n",
    "    plt.figure(figsize=(9, 3))\n",
    "    plt.plot(steps, losses)\n",
    "    plt.xlabel(\"step\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.title(\"training loss\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(f\"step {steps[-1]}, loss {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0000019",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "\n",
    "The trainer saves `best_model.pt` and `last_model.pt` after each epoch. Each checkpoint includes model weights, optimizer state, and the epoch/step/loss metadata needed to resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0000020",
   "metadata": {},
   "outputs": [],
   "source": [
    "CKPT_PATH = Path(\"../checkpoints/best_model.pt\")\n",
    "\n",
    "if not CKPT_PATH.exists():\n",
    "    print(\"no checkpoint yet\")\n",
    "else:\n",
    "    ckpt = torch.load(CKPT_PATH, map_location=\"cpu\")\n",
    "    print(f\"epoch {ckpt['epoch']}, step {ckpt['global_step']:,}, val loss {ckpt['best_val_loss']:.4f}\")\n",
    "    print(f\"keys: {list(ckpt.keys())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
